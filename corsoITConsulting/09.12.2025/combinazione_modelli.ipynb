{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0277b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e split dati...\n",
      "2. Avvio Ottimizzazione Iperparametri (20 trial per modello)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bd3d5f75a94020a7f321b159132508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5849ac9c396e47e0a02e975b074c93c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6db691da9e494c8e2b978c7f1f33c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RMSE Val -> XGB: 0.4549 | CAT: 0.4459 | LGBM: 0.4534\n",
      "\n",
      "3. Configurazione Stacking Regressor con i migliori parametri...\n",
      "Addestramento Stacking (questo passaggio ri-addestra i base models su 5 fold)...\n",
      "\n",
      "4. Valutazione sul Test Set (Hold-out)...\n",
      "----------------------------------------\n",
      "STACKING FINAL RMSE: 0.4298\n",
      "STACKING FINAL R2:   0.8590\n",
      "----------------------------------------\n",
      "Pesi assegnati dal Meta-Modello (Ridge):\n",
      "Intercept: -0.0167\n",
      "XGBoost   : 0.2949\n",
      "CatBoost  : 0.5119\n",
      "LightGBM  : 0.2012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import logging\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Stacking\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Configurazione Silenziosa\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. PREPARAZIONE DATI\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento e split dati...\")\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Split Principale: 80% Train (per Optuna + Stacking), 20% Test (Hold-out finale)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Secondario: Solo per Optuna (per non overfittare sul train set intero durante il tuning)\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. OTTIMIZZAZIONE OPTUNA (LIGHT)\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Avvio Ottimizzazione Iperparametri (20 trial per modello)...\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=20, show_progress_bar=True)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({'n_jobs': -1, 'random_state': 42}) # Reinseriamo params statici\n",
    "\n",
    "# --- CatBoost ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 500, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=20, show_progress_bar=True)\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params.update({'verbose': 0, 'allow_writing_files': False, 'random_state': 42})\n",
    "\n",
    "# --- LightGBM ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=20, show_progress_bar=True)\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "best_lgbm_params.update({'n_jobs': -1, 'verbose': -1, 'random_state': 42})\n",
    "\n",
    "print(f\"\\nBest RMSE Val -> XGB: {study_xgb.best_value:.4f} | CAT: {study_cat.best_value:.4f} | LGBM: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. COSTRUZIONE E TRAINING STACKING REGRESSOR\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n3. Configurazione Stacking Regressor con i migliori parametri...\")\n",
    "\n",
    "# Definizione Level 0 (Base Models)\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params))\n",
    "]\n",
    "\n",
    "# Definizione Level 1 (Meta Model)\n",
    "# RidgeCV trova automaticamente la regolarizzazione (alpha) migliore\n",
    "meta_learner = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,            # 5-Fold per generare le previsioni out-of-fold robuste\n",
    "    n_jobs=-1,\n",
    "    passthrough=False \n",
    ")\n",
    "\n",
    "print(\"Addestramento Stacking (questo passaggio ri-addestra i base models su 5 fold)...\")\n",
    "stacking_regressor.fit(X_train_full, y_train_full)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. VALUTAZIONE FINALE\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Valutazione sul Test Set (Hold-out)...\")\n",
    "\n",
    "# Predizione\n",
    "preds = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Metriche\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"STACKING FINAL RMSE: {rmse:.4f}\")\n",
    "print(f\"STACKING FINAL R2:   {r2:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Ispezione Pesi Meta-Learner\n",
    "print(\"Pesi assegnati dal Meta-Modello (Ridge):\")\n",
    "print(f\"Intercept: {stacking_regressor.final_estimator_.intercept_:.4f}\")\n",
    "model_names = ['XGBoost', 'CatBoost', 'LightGBM']\n",
    "coeffs = stacking_regressor.final_estimator_.coef_\n",
    "\n",
    "for name, coef in zip(model_names, coeffs):\n",
    "    print(f\"{name:<10}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6336fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Stacking\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ff3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Data Loading & Statistical Cleaning...\n",
      "Dataset pulito con IQR: 15203 righe (Rimossi 4445 outlier)\n",
      "2. Creazione Feature (Distanze, Rapporti, Cluster)...\n",
      "    -> Generazione Cluster Geografici (KMeans)...\n",
      "\n",
      "3. Tuning Iperparametri...\n",
      "Best RMSE Val -> XGB: 0.3595 | CAT: 0.3643 | LGBM: 0.3645\n",
      "\n",
      "4. Addestramento Stacking Ensemble (CV=5)...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configurazione Silenziosa\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO E PULIZIA AVANZATA (IQR)\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Data Loading & Statistical Cleaning...\")\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Rimuoviamo il cap artificiale del target (valori >= 5.0)\n",
    "df = df[df['MedHouseVal'] < 5.0]\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    \"\"\"\n",
    "    Rimuove gli outlier usando la regola dell'IQR (Interquartile Range).\n",
    "    Mantiene solo i dati tra Q1 - 1.5*IQR e Q3 + 1.5*IQR.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    indices_to_drop = []\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.30)\n",
    "        Q3 = df_clean[col].quantile(0.70)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identifichiamo gli indici da rimuovere per questa colonna\n",
    "        outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)].index\n",
    "        indices_to_drop.extend(outliers)\n",
    "    \n",
    "    # Rimuoviamo i duplicati degli indici e facciamo il drop\n",
    "    indices_to_drop = list(set(indices_to_drop))\n",
    "    df_clean = df_clean.drop(indices_to_drop)\n",
    "    return df_clean\n",
    "\n",
    "# Applichiamo IQR solo su feature \"fisiche\" soggette a errori di input o anomalie estreme\n",
    "cols_to_clean = ['AveRooms', 'AveBedrms', 'AveOccup', 'MedInc']\n",
    "original_len = len(df)\n",
    "df = remove_outliers_iqr(df, cols_to_clean)\n",
    "print(f\"Dataset pulito con IQR: {len(df)} righe (Rimossi {original_len - len(df)} outlier)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING AVANZATA\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Creazione Feature (Distanze, Rapporti, Cluster)...\")\n",
    "\n",
    "# A. Feature basate su Domain Knowledge\n",
    "df['Bedrms_per_Room'] = df['AveBedrms'] / df['AveRooms']\n",
    "df['Rooms_per_Person'] = df['AveRooms'] / df['AveOccup']\n",
    "# Interazione Reddito * Stanze (Capacità di spesa per spazio)\n",
    "df['Wealth_Capacity'] = df['MedInc'] * df['AveRooms']\n",
    "\n",
    "# B. Feature Geospaziali: Distanza dai centri economici\n",
    "# Coordinate approssimative (Lat, Lon)\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "\n",
    "def dist_calc(row, city_coords):\n",
    "    # Distanza Euclidea semplificata (sufficiente per ML su scala locale)\n",
    "    return np.sqrt((row['Latitude'] - city_coords[0])**2 + (row['Longitude'] - city_coords[1])**2)\n",
    "\n",
    "df['Dist_SF'] = df.apply(lambda x: dist_calc(x, sf_coords), axis=1)\n",
    "df['Dist_LA'] = df.apply(lambda x: dist_calc(x, la_coords), axis=1)\n",
    "# Feature: Distanza minima dal centro metropolitano più vicino\n",
    "df['Dist_Min_Metro'] = df[['Dist_SF', 'Dist_LA']].min(axis=1)\n",
    "\n",
    "def point_line_distance(x0, y0, a, b, c):\n",
    "    return abs(a * x0 + b * y0 + c) / np.sqrt(a*a + b*b)\n",
    "df[\"DistToCoast\"] = point_line_distance(df['Longitude'], df['Latitude'], 1.25, 1, 116)\n",
    "\n",
    "# Separazione X e y\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "df['IncTot'] = df['MedInc'] * df['Population']\n",
    "df['IncPerPers'] = df['MedInc'] / df['Population']\n",
    "df['PopulationPerHousehold'] = df['Population'] / df['AveOccup'] \n",
    "\n",
    "df['MedInc_log'] = np.log1p(df['MedInc'])          # log(MedInc + 1)\n",
    "df['Population_log'] = np.log1p(df['Population'])  # log(Population + 1)\n",
    "df['AveRooms_log'] = np.log1p(df['AveRooms'])\n",
    "df['AveBedrms_log'] = np.log1p(df['AveBedrms'])\n",
    "\n",
    "# Split Train/Test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# C. KMeans Clustering (Gestito correttamente per evitare Leakage)\n",
    "# Fit solo su Train, Transform su Train e Test\n",
    "print(\"    -> Generazione Cluster Geografici (KMeans)...\")\n",
    "scaler_geo = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "\n",
    "# Prendiamo solo lat/long\n",
    "train_geo = X_train_full[['Latitude', 'Longitude']]\n",
    "test_geo = X_test[['Latitude', 'Longitude']]\n",
    "\n",
    "# Scaliamo (KMeans è sensibile alla scala)\n",
    "train_geo_scaled = scaler_geo.fit_transform(train_geo)\n",
    "test_geo_scaled = scaler_geo.transform(test_geo)\n",
    "\n",
    "# Creiamo la feature cluster\n",
    "X_train_full['Geo_Cluster'] = kmeans.fit_predict(train_geo_scaled)\n",
    "X_test['Geo_Cluster'] = kmeans.predict(test_geo_scaled)\n",
    "\n",
    "# Assicuriamoci che Geo_Cluster sia trattata come categorica o intera\n",
    "X_train_full['Geo_Cluster'] = X_train_full['Geo_Cluster'].astype(int)\n",
    "X_test['Geo_Cluster'] = X_test['Geo_Cluster'].astype(int)\n",
    "\n",
    "\n",
    "# Split per Optuna (dal Train set pulito)\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. OTTIMIZZAZIONE OPTUNA\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n3. Tuning Iperparametri...\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log = True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, model.predict(X_opt_val)))\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=15) # 15 trial per velocità\n",
    "best_xgb = study_xgb.best_params\n",
    "best_xgb.update({'n_jobs': -1, 'random_state': 42})\n",
    "\n",
    "# --- CatBoost ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log = True),\n",
    "        'depth': trial.suggest_int('depth', 4, 7),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_opt_train, y_opt_train, cat_features=['Geo_Cluster'])\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, model.predict(X_opt_val)))\n",
    "\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=15)\n",
    "best_cat = study_cat.best_params\n",
    "best_cat.update({'verbose': 0, 'allow_writing_files': False, 'random_state': 42})\n",
    "\n",
    "# --- LightGBM ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 600, 1200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5, log = True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    # LightGBM gestisce le categorie (Geo_Cluster) se specificate\n",
    "    cat_features = ['Geo_Cluster']\n",
    "    X_train_lgbm = X_opt_train.copy()\n",
    "    X_val_lgbm = X_opt_val.copy()\n",
    "    X_train_lgbm[cat_features] = X_train_lgbm[cat_features].astype('category')\n",
    "    X_val_lgbm[cat_features] = X_val_lgbm[cat_features].astype('category')\n",
    "    \n",
    "    model.fit(X_train_lgbm, y_opt_train, categorical_feature=cat_features)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, model.predict(X_val_lgbm)))\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=15)\n",
    "best_lgbm = study_lgbm.best_params\n",
    "best_lgbm.update({'n_jobs': -1, 'verbose': -1, 'random_state': 42})\n",
    "\n",
    "print(f\"Best RMSE Val -> XGB: {study_xgb.best_value:.4f} | CAT: {study_cat.best_value:.4f} | LGBM: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. STACKING REGRESSOR\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Addestramento Stacking Ensemble (CV=5)...\")\n",
    "\n",
    "# Prepariamo i dati per Stacking (gestione delle feature categoriali per lgbm e cat)\n",
    "X_train_stack = X_train_full.copy()\n",
    "X_test_stack = X_test.copy()\n",
    "cat_features = ['Geo_Cluster']\n",
    "X_train_stack[cat_features] = X_train_stack[cat_features].astype('category')\n",
    "X_test_stack[cat_features] = X_test_stack[cat_features].astype('category')\n",
    "\n",
    "estimators = [\n",
    "    ('xgb', XGBRegressor(**best_xgb, enable_categorical=True)),\n",
    "    ('cat', CatBoostRegressor(**best_cat, cat_features=cat_features)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm, categorical_feature=cat_features))\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4096de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Valutazione Finale, Coefficienti Stacking e Analisi Importanza Feature...\n",
      "\n",
      "--- RISULTATI FINALI STACKING ENSEMBLE ---\n",
      "Final RMSE (Test Set): 0.3525\n",
      "Final R-squared (Test Set): 0.8487\n",
      "------------------------------------------\n",
      "\n",
      "--- COEFFICIENTI DEL MODELLO DI STACKING (RIDGE CV) ---\n",
      "Coefficiente XGB : 0.3845\n",
      "Coefficiente CAT : 0.3632\n",
      "Coefficiente LGBM: 0.2627\n",
      "-------------------------------------------------------\n",
      "\n",
      "Top 5 Feature più importanti (Basate su XGBoost Base Estimator):\n",
      "Geo_Cluster         0.318957\n",
      "DistToCoast         0.124726\n",
      "MedInc              0.083909\n",
      "Rooms_per_Person    0.064340\n",
      "Dist_Min_Metro      0.055833\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# RidgeCV usa la Generalized Cross-Validation per trovare l'alpha ottimale in modo efficiente\n",
    "\n",
    "meta_learner = RidgeCV(alphas=[0.1, 0.4, 1.0, 4.0, 10.0])\n",
    "\n",
    "stacking = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False \n",
    ")\n",
    "\n",
    "# Addestramento finale su tutto il set di training\n",
    "stacking.fit(X_train_stack, y_train_full)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. RISULTATI FINALI, COEFFICIENTI e FEATURE IMPORTANCE\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n5. Valutazione Finale, Coefficienti Stacking e Analisi Importanza Feature...\")\n",
    "\n",
    "# 5.1. Calcolo e Stampa delle Metriche Finali\n",
    "y_pred = stacking.predict(X_test_stack)\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- RISULTATI FINALI STACKING ENSEMBLE ---\")\n",
    "print(f\"Final RMSE (Test Set): {final_rmse:.4f}\")\n",
    "print(f\"Final R-squared (Test Set): {final_r2:.4f}\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "\n",
    "# 5.2. Stampa dei Coefficienti del Meta-Learner (RidgeCV)\n",
    "meta_coef = stacking.final_estimator_.coef_\n",
    "base_names = [name for name, _ in stacking.estimators]\n",
    "\n",
    "print(\"\\n--- COEFFICIENTI DEL MODELLO DI STACKING (RIDGE CV) ---\")\n",
    "for name, coef in zip(base_names, meta_coef):\n",
    "    print(f\"Coefficiente {name.upper():<4}: {coef:.4f}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "# Nota: La somma dei coefficienti dovrebbe essere vicina a 1, indicando una media ponderata.\n",
    "\n",
    "\n",
    "# 5.3. Analisi Importanza Feature\n",
    "xgb_temp = stacking.estimators_[0]\n",
    "feature_names = X_train_full.columns \n",
    "\n",
    "importances = pd.Series(xgb_temp.feature_importances_, index=feature_names)\n",
    "\n",
    "print(\"\\nTop 5 Feature più importanti (Basate su XGBoost Base Estimator):\")\n",
    "print(importances.sort_values(ascending=False).head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
