{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Caricamento e Pulizia Dati...\n",
      "   -> Dataset pulito: (1825, 9)\n",
      "2. Generazione Feature 'Combo'...\n",
      "   -> Totale Feature Generate: 105\n",
      "3. Selezione Feature tramite GPU...\n",
      "   -> Feature Sopravvissute: 38\n",
      "\n",
      "4. Tuning Iperparametri con Optuna e Early Stopping...\n",
      "   -> Optimizing XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-10 12:19:16,126] Trial 0 failed with parameters: {'learning_rate': 0.09467957867217243, 'depth': 6, 'l2_leaf_reg': 5.153561142488177, 'subsample': 0.6809025927081477} because of the following error: CatBoostError(\"catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\m-rog\\AppData\\Local\\Temp\\ipykernel_7292\\1660013717.py\", line 194, in objective_cat\n",
      "    model.fit(\n",
      "    ~~~~~~~~~^\n",
      "        X_opt_train, y_opt_train,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        eval_set=[(X_opt_val, y_opt_val)],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        early_stopping_rounds=100\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                     use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                     verbose_eval, metric_period, silent, early_stopping_rounds,\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                     save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "    ~~~~~~~~~~~^\n",
      "        train_pool,\n",
      "        ^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        train_params[\"init_model\"]\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "_catboost.CatBoostError: catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'\n",
      "[W 2025-12-10 12:19:16,127] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Optimizing CatBoost...\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   -> Optimizing CatBoost...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m study_cat \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m study_cat\u001b[38;5;241m.\u001b[39moptimize(objective_cat, n_trials\u001b[38;5;241m=\u001b[39mN_TRIALS)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# --- C. LightGBM Optimization (New Callback API) ---\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobjective_lgbm\u001b[39m(trial):\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m     _optimize(\n\u001b[0;32m    491\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    492\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    493\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    494\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    495\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    496\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    497\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    498\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    499\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    500\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m         _optimize_sequential(\n\u001b[0;32m     68\u001b[0m             study,\n\u001b[0;32m     69\u001b[0m             func,\n\u001b[0;32m     70\u001b[0m             n_trials,\n\u001b[0;32m     71\u001b[0m             timeout,\n\u001b[0;32m     72\u001b[0m             catch,\n\u001b[0;32m     73\u001b[0m             callbacks,\n\u001b[0;32m     74\u001b[0m             gc_after_trial,\n\u001b[0;32m     75\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     76\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     77\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     78\u001b[0m         )\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    258\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    261\u001b[0m ):\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[2], line 194\u001b[0m, in \u001b[0;36mobjective_cat\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    177\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4000\u001b[39m,\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    190\u001b[0m }\n\u001b[0;32m    192\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 194\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    195\u001b[0m     X_opt_train, y_opt_train,\n\u001b[0;32m    196\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m[(X_opt_val, y_opt_val)],\n\u001b[0;32m    197\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m    198\u001b[0m )\n\u001b[0;32m    200\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_opt_val)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_opt_val, preds))\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\catboost\\core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, graph, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline,\n\u001b[0;32m   5874\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5875\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5876\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   2411\u001b[0m         train_pool,\n\u001b[0;32m   2412\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2413\u001b[0m         params,\n\u001b[0;32m   2414\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2415\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2416\u001b[0m     )\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\m-rog\\anaconda3\\Lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5023\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5072\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: catboost/cuda/cuda_lib/cuda_manager.cpp:201: Condition violated: `State == nullptr'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import warnings\n",
    "import itertools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset e Preprocessing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Modelli\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Configurazione\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. CARICAMENTO E PULIZIA DATI\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Caricamento e Pulizia Dati...\")\n",
    "start_global = time.time()\n",
    "\n",
    "data = fetch_california_housing()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "df_full = pd.concat([X, y], axis=1)\n",
    "df = df_full.sample(frac=0.10, random_state=42)\n",
    "\n",
    "# Rimuoviamo il cap a 5.0\n",
    "df = df[df['MedHouseVal'] < 5.0]\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    indices_to_drop = []\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df_clean[(df_clean[col] < Q1 - 2.0*IQR) | (df_clean[col] > Q3 + 2.0*IQR)].index\n",
    "        indices_to_drop.extend(outliers)\n",
    "    return df_clean.drop(list(set(indices_to_drop)))\n",
    "\n",
    "cols_clean = ['AveRooms', 'AveBedrms', 'AveOccup', 'MedInc']\n",
    "df = remove_outliers_iqr(df, cols_clean)\n",
    "print(f\"   -> Dataset pulito: {df.shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING MASSIVA\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Generazione Feature 'Combo'...\")\n",
    "\n",
    "def generate_comprehensive_features(df_input, cols_to_combine):\n",
    "    df_eng = df_input.copy()\n",
    "    math_cols = [c for c in cols_to_combine if c not in ['Latitude', 'Longitude', 'Geo_Cluster']]\n",
    "    \n",
    "    # Trucco Rotazione Coordinate per Alberi\n",
    "    df_eng['Rot_45_LatLon'] = df_eng['Latitude'] + df_eng['Longitude']\n",
    "    df_eng['Rot_N45_LatLon'] = df_eng['Latitude'] - df_eng['Longitude']\n",
    "\n",
    "    # A. LOGARITMI\n",
    "    for col in math_cols:\n",
    "        if df_eng[col].min() >= 0:\n",
    "            df_eng[f'LOG_{col}'] = np.log1p(df_eng[col])\n",
    "\n",
    "    # B. MOLTIPLICAZIONI\n",
    "    for col1, col2 in itertools.combinations(math_cols, 2):\n",
    "        col_name = f'MULT_{col1}_x_{col2}'\n",
    "        df_eng[col_name] = df_eng[col1] * df_eng[col2]\n",
    "\n",
    "    # C. DIVISIONI\n",
    "    for col1, col2 in itertools.permutations(math_cols, 2):\n",
    "        col_name = f'RATIO_{col1}_div_{col2}'\n",
    "        df_eng[col_name] = df_eng[col1] / (df_eng[col2] + 1e-5)\n",
    "\n",
    "    return df_eng\n",
    "\n",
    "# Geo Features Base\n",
    "sf_coords = (37.7749, -122.4194)\n",
    "la_coords = (34.0522, -118.2437)\n",
    "df['Dist_SF'] = np.sqrt((df['Latitude'] - sf_coords[0])**2 + (df['Longitude'] - sf_coords[1])**2)\n",
    "df['Dist_LA'] = np.sqrt((df['Latitude'] - la_coords[0])**2 + (df['Longitude'] - la_coords[1])**2)\n",
    "\n",
    "coords = df[['Latitude', 'Longitude']]\n",
    "kmeans = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "df['Geo_Cluster'] = kmeans.fit_predict(StandardScaler().fit_transform(coords))\n",
    "\n",
    "X = df.drop('MedHouseVal', axis=1)\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "cols_for_math = [c for c in X.columns if c != 'Geo_Cluster']\n",
    "X_full = generate_comprehensive_features(X, cols_for_math)\n",
    "X_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_full.fillna(0, inplace=True)\n",
    "print(f\"   -> Totale Feature Generate: {X_full.shape[1]}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. SELEZIONE FEATURE CON XGBOOST GPU\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Selezione Feature tramite GPU...\")\n",
    "scaler = RobustScaler()\n",
    "# Nota: fit non richiede scaling per alberi, ma utile averlo per prassi\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "\n",
    "selector_model = XGBRegressor(\n",
    "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "    tree_method='hist', device='gpu', n_jobs=1, random_state=42\n",
    ")\n",
    "selector_model.fit(X_full, y)\n",
    "selection = SelectFromModel(selector_model, prefit=True, threshold='1.25*median')\n",
    "\n",
    "X_selected = X_full.loc[:, selection.get_support()]\n",
    "print(f\"   -> Feature Sopravvissute: {X_selected.shape[1]}\")\n",
    "\n",
    "# Split Principale (Train / Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. OPTUNA REALE CON EARLY STOPPING CORRETTO\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n4. Tuning Iperparametri con Optuna e Early Stopping...\")\n",
    "\n",
    "# Creiamo un set di validazione SOLO per Optuna per monitorare l'early stopping\n",
    "X_opt_train, X_opt_val, y_opt_train, y_opt_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "N_TRIALS = 20  # Numero di trial (aumentare per risultati migliori)\n",
    "\n",
    "# --- A. XGBoost Optimization ---\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,  # Alto, tanto ferma l'early stopping\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0, log=True),\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'gpu',\n",
    "        'n_jobs': 1,\n",
    "        'random_state': 42,\n",
    "        # Early Stopping nel costruttore per versioni recenti (o gestito in fit)\n",
    "        'early_stopping_rounds': 100\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    # Verbose=False sopprime i log, eval_set serve per l'early stopping\n",
    "    model.fit(\n",
    "        X_opt_train, y_opt_train,\n",
    "        eval_set=[(X_opt_val, y_opt_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\"   -> Optimizing XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS)\n",
    "\n",
    "# --- B. CatBoost Optimization ---\n",
    "def objective_cat(trial):\n",
    "    params = {\n",
    "        'iterations': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 6, 12),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n",
    "        'task_type': 'GPU',\n",
    "        'devices': '0',\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'RMSE'\n",
    "    }\n",
    "    \n",
    "    model = CatBoostRegressor(**params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_opt_train, y_opt_train,\n",
    "        eval_set=[(X_opt_val, y_opt_val)],\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\"   -> Optimizing CatBoost...\")\n",
    "study_cat = optuna.create_study(direction='minimize')\n",
    "study_cat.optimize(objective_cat, n_trials=N_TRIALS)\n",
    "\n",
    "# --- C. LightGBM Optimization (New Callback API) ---\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': 4000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 200),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.95),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.95),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10),\n",
    "        'device': 'gpu', # Assicurati di avere LGBM compilato per GPU, altrimenti usa 'cpu'\n",
    "        'n_jobs': 1,\n",
    "        'verbosity': -1,\n",
    "        'random_state': 42,\n",
    "        'metric': 'rmse'\n",
    "    }\n",
    "    \n",
    "    model = LGBMRegressor(**params)\n",
    "    \n",
    "    # NEW API: Usiamo callbacks invece di early_stopping_rounds in .fit()\n",
    "    callbacks = [\n",
    "        early_stopping(stopping_rounds=100, verbose=False),\n",
    "        log_evaluation(period=0) # Zittisce l'output\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        model.fit(\n",
    "            X_opt_train, y_opt_train,\n",
    "            eval_set=[(X_opt_val, y_opt_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Fallback CPU se GPU crasha o non presente\n",
    "        params['device'] = 'cpu'\n",
    "        model = LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_opt_train, y_opt_train,\n",
    "            eval_set=[(X_opt_val, y_opt_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "    preds = model.predict(X_opt_val)\n",
    "    return np.sqrt(mean_squared_error(y_opt_val, preds))\n",
    "\n",
    "print(\"   -> Optimizing LightGBM...\")\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=N_TRIALS)\n",
    "\n",
    "# Recupero i migliori parametri\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({'n_estimators': 2000, 'tree_method': 'hist', 'device': 'gpu', 'n_jobs': 1, 'random_state': 42})\n",
    "# Rimuoviamo early_stopping_rounds dai params per il fit finale (opzionale, ma pulito)\n",
    "if 'early_stopping_rounds' in best_xgb_params: del best_xgb_params['early_stopping_rounds']\n",
    "\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params.update({'iterations': 2000, 'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'random_state': 42})\n",
    "\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "best_lgbm_params.update({'n_estimators': 2000, 'device': 'gpu', 'n_jobs': 1, 'verbosity': -1, 'random_state': 42})\n",
    "\n",
    "print(\"\\n--- Optuna Completato. Parametri migliori trovati. ---\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. SMART SOFT MODEL SELECTOR (Mixture of Experts)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n5. Costruzione Smart Soft Selector (Mixture of Experts)...\")\n",
    "\n",
    "class SmartSoftModelSelector(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators, selector_model=None):\n",
    "        self.estimators = estimators # Lista di tuple ('nome', modello)\n",
    "        # Usiamo XGBClassifier su GPU come selettore per massima velocità\n",
    "        self.selector_model = selector_model if selector_model else XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            tree_method='hist', device='gpu', n_jobs=1, random_state=42\n",
    "        )\n",
    "        self.model_names = [name for name, _ in estimators]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # A. Addestramento Modelli Base\n",
    "        print(\"   -> Training modelli base (Esperti) su tutto il Train Set...\")\n",
    "        self.fitted_estimators_ = []\n",
    "        for name, model in self.estimators:\n",
    "            # Nota: Qui non usiamo Early Stopping perché vorremmo usare tutto il train set.\n",
    "            # Usiamo i parametri ottimizzati da Optuna che sono robusti.\n",
    "            model.fit(X, y)\n",
    "            self.fitted_estimators_.append(model)\n",
    "            \n",
    "        # B. Generazione OOF Predictions (Chi sbaglia meno dove?)\n",
    "        print(\"   -> Generazione dati per il Gating Network...\")\n",
    "        errors = pd.DataFrame()\n",
    "        \n",
    "        # Nota: Usiamo n_jobs=1 in cross_val_predict perché i modelli usano GPU internamente\n",
    "        for name, model in self.estimators:\n",
    "            # cross_val_predict è essenziale per evitare leakage\n",
    "            oof_preds = cross_val_predict(model, X, y, cv=5, n_jobs=1)\n",
    "            errors[name] = np.abs(y - oof_preds) # Errore Assoluto\n",
    "            \n",
    "        # C. Creazione Target per il Selettore\n",
    "        y_best_model_idx = errors.idxmin(axis=1).apply(lambda x: self.model_names.index(x))\n",
    "        \n",
    "        # D. Addestramento Selettore (Gating Network)\n",
    "        print(\"   -> Training del Selettore (Gating Network)...\")\n",
    "        self.selector_model.fit(X, y_best_model_idx)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # A. Predizioni Base\n",
    "        base_preds = np.column_stack([model.predict(X) for model in self.fitted_estimators_])\n",
    "        # B. Pesi Soft\n",
    "        weights = self.selector_model.predict_proba(X)\n",
    "        # C. Media Ponderata Dinamica\n",
    "        final_pred = np.sum(base_preds * weights, axis=1)\n",
    "        return final_pred\n",
    "\n",
    "# Istanziamo i modelli base con i Best Params di Optuna\n",
    "estimators_list = [\n",
    "    ('xgb', XGBRegressor(**best_xgb_params)),\n",
    "    ('cat', CatBoostRegressor(**best_cat_params)),\n",
    "    ('lgbm', LGBMRegressor(**best_lgbm_params)) \n",
    "]\n",
    "\n",
    "# Creiamo e addestriamo il sistema MoE\n",
    "moe_model = SmartSoftModelSelector(estimators=estimators_list)\n",
    "moe_model.fit(X_train, y_train)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. AGGIUNTA DEL CORRETTORE DEI RESIDUI (Residual Learning)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n6. Training Correttore dei Residui...\")\n",
    "\n",
    "class ResidualCorrectedMoE(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_moe_model, corrector_model=None):\n",
    "        self.base_moe_model = base_moe_model\n",
    "        # Usiamo un CatBoost leggero come correttore\n",
    "        self.corrector_model = corrector_model if corrector_model else CatBoostRegressor(\n",
    "            iterations=500,          \n",
    "            depth=6, \n",
    "            learning_rate=0.03,\n",
    "            l2_leaf_reg=5,           \n",
    "            task_type='GPU', \n",
    "            devices='0',\n",
    "            verbose=0,\n",
    "            allow_writing_files=False,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # 2. Generiamo le predizioni OOF per calcolare i residui onesti\n",
    "        print(\"   -> Calcolo residui OOF (può richiedere tempo)...\")\n",
    "        oof_preds = cross_val_predict(self.base_moe_model, X, y, cv=5, n_jobs=1)\n",
    "        \n",
    "        # 3. Calcolo dell'Errore\n",
    "        residuals = y - oof_preds\n",
    "        \n",
    "        print(f\"      Media Residui: {residuals.mean():.4f}\")\n",
    "        print(f\"      Deviaz. Std Residui: {residuals.std():.4f}\")\n",
    "        \n",
    "        # 4. Addestriamo il Correttore\n",
    "        print(\"   -> Training del Correttore...\")\n",
    "        self.corrector_model.fit(X, residuals)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        base_pred = self.base_moe_model.predict(X)\n",
    "        correction = self.corrector_model.predict(X)\n",
    "        return base_pred + (1.0 * correction)\n",
    "\n",
    "# Costruzione Finale\n",
    "# Nota: passiamo il moe_model già addestrato, ma la classe ResidualCorrectedMoE\n",
    "# userà cross_val_predict che internamente farà cloni e fit su fold.\n",
    "final_system = ResidualCorrectedMoE(base_moe_model=moe_model)\n",
    "final_system.fit(X_train, y_train)\n",
    "\n",
    "# Valutazione\n",
    "y_pred_corrected = final_system.predict(X_test)\n",
    "\n",
    "final_rmse_corr = np.sqrt(mean_squared_error(y_test, y_pred_corrected))\n",
    "final_r2_corr = r2_score(y_test, y_pred_corrected)\n",
    "\n",
    "print(f\"\\n==========================================\")\n",
    "print(f\" RISULTATI FINALI OTTIMIZZATI\")\n",
    "print(f\"==========================================\")\n",
    "print(f\" RMSE: {final_rmse_corr:.5f}\")\n",
    "print(f\" R^2 : {final_r2_corr:.5f}\")\n",
    "print(f\"==========================================\")\n",
    "\n",
    "# Plot veloce\n",
    "corrections_test = final_system.corrector_model.predict(X_test)\n",
    "base_preds_test = final_system.base_moe_model.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=base_preds_test, y=corrections_test, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title(\"Bias Correction vs Price (Optimized Models)\")\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Correction\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
